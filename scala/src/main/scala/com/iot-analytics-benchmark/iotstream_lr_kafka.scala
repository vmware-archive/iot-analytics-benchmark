/*
iotstream_lr_kafka.scala: Spark streaming program to analyze data generated by sim_sensors_lr_kafka.scala with Logistic Regression using Kafka input
Outputs prediction of model based on input data
  
In one window:
  $ scala -cp <path>iotstream_<scala version>-<code version>.jar:/root/kafka/libs/* com.iotstream.sim_sensors_lr_kafka n_sensors average_sensor_events_per_second \ 
    total_events kafka_server_list kafka_topic
  
In another window:
  $ spark-submit --name iotstream_lr_kafka --class com.iotstream.iotstream_lr_kafka <path>iotstream_<scala version>-<code version>.jar n_sensors reporting_interval \
    kafka_server_list kafka_topic HDFS_or_S3 HDFS_path_or_S3_bucket modelname

Copyright (c) 2018 VMware, Inc.

This product is licensed to you under the Apache 2.0 license (the "License").  You may not use this product except in compliance with the Apache 2.0 License.

This product may include a number of subcomponents with separate copyright notices and license terms. Your use of these subcomponents is subject to the terms and conditions of the subcomponent's license, as noted in the LICENSE file.
*/

package com.iotstream

import kafka.serializer.StringDecoder
import org.apache.spark._
import org.apache.spark.rdd.RDD
import org.apache.spark.SparkContext._
import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.classification.LogisticRegressionModel
import org.apache.spark.streaming._
import org.apache.spark.streaming.kafka._
import java.time._

object iotstream_lr_kafka {
  def main(args: Array[String]) {
  
    if (args.length != 7) {
      System.err.println("Usage: spark-submit --name iotstream_lr_kafka --class com.iotstream.iotstream_lr_kafka <path>iotstream_<scala version>-<code version>.jar n_sensors reporting_interval kafka_server_list kafka_topic HDFS_or_S3 HDFS_path_or_S3_bucket modelname")
      System.exit(-1)
    }
  
    val n_sensors = args(0).toInt
    val reporting_interval = args(1).toLong
    val kafka_server_list = args(2)
    val topic = args(3)

    val modelname = {
      if (args(4).capitalize == "S3") "s3a://%s/%s".format(args(5), args(6))
      else "%s/%s".format(args(5), args(6))
    }
  
    println("%s: Analyzing stream of input from kafka topic %s with kafka server(s) %s, using LR model %s, with %d second intervals".format(Instant.now.toString, topic, kafka_server_list, modelname, reporting_interval))

    // Initialize streaming for specified reporting interval
    val conf = new SparkConf().setAppName("iotstream_lr_kafka")
    val sc = new SparkContext(conf)
    val interval = sc.accumulator(0)
    val empty_intervals = sc.accumulator(0)
    val events  = sc.accumulator(0)
    val ssc = new StreamingContext(sc, Seconds(reporting_interval))
    val topicsSet = List(topic).toSet
    val kafkaParams = Map[String, String]("bootstrap.servers" -> kafka_server_list)
    val sensor_stream = KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder](ssc, kafkaParams, topicsSet)
    // Load pre-computed model
    val model = LogisticRegressionModel.load(sc, modelname)

    //  Initialize features to <number of sensors>-length array, filled with neutral initial sensor value
    val features = Array.fill(n_sensors)(0.5)

    def run_model(rdd: RDD[String]): Unit = {
      var last_batch = false
      // Input rdd consists of lines of sensor inputs received during that particular batch
      // This code combines the lines in each batch into a feature vector containing the latest sensor values
      // Don't process empty batches
      if (rdd.count == 0) {
        empty_intervals.add(1)
        println("No input")
      } 
      else {
        // Each line of input has format Timestamp (string), sensor number (integer), sensor name (string), sensor value (float), eg
        // 2017-12-14T22:22:43.895Z,19,Sensor 19,0.947640
        // Split each line into its fields, filter out any lines that are not the expected length, then
        //  create a list of tuples with each tuple representing (sensor number, sensor value)
        val input = rdd.map(_.split(",")).filter(_.length == 4).map(list => (list(1).toInt, list(3).toFloat)).collect
        // Read input into features vector. If a sensor was not read during this interval its current value will persist. A negative sensor value means end of events.
        for (t <- input) {
          if (t._1 < 0) last_batch = true
          else features(t._1-1) = t._2
        }
        // If model predicts True warn user in red
        if (model.predict(Vectors.dense(features)) > 0) {
          println("\033[31m%s: Interval %d: Attention needed (%d sensor events in interval)\033[0m".format(Instant.now.toString, interval.value, input.length))
        } else {
          println("%s: Interval %d: Everything is OK (%d sensor events in interval)".format(Instant.now.toString, interval.value, input.length))
        }
        interval.add(1)
        events.add(input.length)
      }
      if (last_batch) ssc.stop()
    }

    // Run model on each batch
    //sensor_stream.print()
    // Discretized stream consists of (offset, RDD) tuples; discard offset
    sensor_stream.map(_._2).foreachRDD(run_model(_))
     
    // Start reading streaming data
    ssc.start()
    val start_time = System.nanoTime
    ssc.awaitTermination()
    val finish_time = System.nanoTime
    val elapsed_time = (finish_time - start_time)/1000000000.0  - empty_intervals.value*reporting_interval - 1  // Subtract off time waiting for events and 1 sec for termination
    println("\n%s: %d events received in %.1f seconds (%d intervals), or %.0f sensor events/second\n".format(Instant.now.toString, events.value-1, elapsed_time, interval.value, (events.value-1).toFloat/elapsed_time))
  }
}
